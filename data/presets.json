{
  "presets": [
    {
      "name": "Llama-3-8B",
      "architecture": "llama",
      "params_billion": 8,
      "hidden_size": 4096,
      "num_layers": 32,
      "num_attention_heads": 32,
      "num_kv_heads": 8,
      "intermediate_size": 14336,
      "vocab_size": 128256,
      "max_seq_length": 8192,
      "is_moe": false,
      "activation": "silu"
    },
    {
      "name": "Llama-3-70B",
      "architecture": "llama",
      "params_billion": 70,
      "hidden_size": 8192,
      "num_layers": 80,
      "num_attention_heads": 64,
      "num_kv_heads": 8,
      "intermediate_size": 28672,
      "vocab_size": 128256,
      "max_seq_length": 8192,
      "is_moe": false,
      "activation": "silu"
    },
    {
      "name": "Llama-3.1-405B",
      "architecture": "llama",
      "params_billion": 405,
      "hidden_size": 16384,
      "num_layers": 126,
      "num_attention_heads": 128,
      "num_kv_heads": 8,
      "intermediate_size": 53248,
      "vocab_size": 128256,
      "max_seq_length": 131072,
      "is_moe": false,
      "activation": "silu"
    },
    {
      "name": "Mixtral-8x7B",
      "architecture": "mixtral",
      "params_billion": 46.7,
      "active_params_billion": 12.9,
      "hidden_size": 4096,
      "num_layers": 32,
      "num_attention_heads": 32,
      "num_kv_heads": 8,
      "intermediate_size": 14336,
      "vocab_size": 32000,
      "max_seq_length": 32768,
      "is_moe": true,
      "num_experts": 8,
      "num_experts_per_tok": 2,
      "activation": "silu"
    },
    {
      "name": "Mixtral-8x22B",
      "architecture": "mixtral",
      "params_billion": 141,
      "active_params_billion": 39,
      "hidden_size": 6144,
      "num_layers": 56,
      "num_attention_heads": 48,
      "num_kv_heads": 8,
      "intermediate_size": 16384,
      "vocab_size": 32000,
      "max_seq_length": 65536,
      "is_moe": true,
      "num_experts": 8,
      "num_experts_per_tok": 2,
      "activation": "silu"
    },
    {
      "name": "GPT-3-175B",
      "architecture": "gpt",
      "params_billion": 175,
      "hidden_size": 12288,
      "num_layers": 96,
      "num_attention_heads": 96,
      "num_kv_heads": 96,
      "intermediate_size": 49152,
      "vocab_size": 50257,
      "max_seq_length": 2048,
      "is_moe": false,
      "activation": "gelu"
    },
    {
      "name": "Qwen2-72B",
      "architecture": "qwen",
      "params_billion": 72,
      "hidden_size": 8192,
      "num_layers": 80,
      "num_attention_heads": 64,
      "num_kv_heads": 8,
      "intermediate_size": 29568,
      "vocab_size": 152064,
      "max_seq_length": 131072,
      "is_moe": false,
      "activation": "silu"
    },
    {
      "name": "DeepSeek-V2-236B",
      "architecture": "deepseek",
      "params_billion": 236,
      "active_params_billion": 21,
      "hidden_size": 5120,
      "num_layers": 60,
      "num_attention_heads": 128,
      "num_kv_heads": 128,
      "intermediate_size": 12288,
      "vocab_size": 102400,
      "max_seq_length": 128000,
      "is_moe": true,
      "num_experts": 160,
      "num_experts_per_tok": 6,
      "activation": "silu"
    }
  ]
}
